<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Multi-task learning with object point cloud representations enable strong generalization in dexterous manipulation.">
  <meta name="keywords" content="Dexterous Manipulation, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>D-MASTER: Mask Annealed Transformer for
    Unsupervised Domain Adaptation in Breast
    Cancer Detection from Mammograms</title>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<style>
  .logo-container {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 10px;
  }
  .logo-container img {
      border: none;
      width: 160px; /* Set the width you want */
      height: auto; /* Maintain aspect ratio */
  }
  .content-between-logos {
      flex: 1;
      text-align: center;
  }
  .publication-details, .publication-links {
      margin: 10px 0;
  }
</style>

<body>
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><b>TransFed:</b>: A way to epitomize Focal Modulation using Transformer-based
Federated Learning</h1>
                    <h3 class="title is-4 conference-authors">
                        <div class="logo-container">
                            <a target="_blank" href="https://conferences.miccai.org/2024/en/">
                                <img src="static/images/miccai2023-logo.png" alt="MICCAI 2024">
                            </a>
                            <div class="content-between-logos">
                                <div class="publication-details">
                                    <div class="is-size-5 publication-authors">
                                        <span class="author-block">
                                            <a href="https://www.tajamulashraf.com/">Tajamul Ashraf</a><sup>1</sup>,</span>
                                        <span class="author-block">
                                            <a href="https://www.linkedin.com/in/fuzayil-mir/">Fuzayil Bin Afzal Mir</a><sup>2</sup>,
                                        </span>
                                            <a href="https://sites.google.com/nitsri.net/iqraaltaf">Iqra Altaf Gillani</a><sup>2</sup>,
                                        </span>
                                    </div>
                                    <div class="is-size-5 publication-authors">
                                        <span class="author-block"><sup>1</sup>Indian Institute of Technology Delhi,</span>
                                        <span class="author-block"><sup>2</sup>National Institute of Technology Delhi,</span>
                                    </div>
                                </div>
                                <div class="publication-links">
                                    <span class="link-block">
                                        <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.pdf" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://github.com/Tajamul21/HF-Fed" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2407.06585v1" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://drive.google.com/drive/folders/1GT_1mkL2L_xcEA14375VSci2vQBWDh_h?usp=drive_link" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa fa-database"></i>
                                            </span>
                                            <span>Data</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                            <a target="_blank" href="https://deep-breath-miccai.github.io/#">
                                <img src="static/images/Deep-Breath-logo.png" alt="Deep Breath">
                            </a>
                        </div>
                    </h3>
                </div>
            </div>
        </div>
    </div>
</section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Federated learning has emerged as a promising
paradigm for collaborative machine learning, enabling
multiple clients to train a model while preserving data privacy jointly. Tailored federated learning takes this concept
further by accommodating client heterogeneity and facilitating the learning of personalized models. While the utilization of transformers within federated learning has attracted significant interest, there remains a need to investigate the effects of federated learning algorithms on the latest focal modulation-based transformers. In this paper, we
investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal
Modulation, particularly in scenarios with heterogeneous
data. To address this challenge, we propose TransFed, a
novel transformer-based federated learning framework that
not only aggregates model parameters but also learns tailored Focal Modulation for each client. Instead of employing a conventional customization mechanism that maintains client-specific focal modulation layers locally, we introduce a learn-to-tailor approach that fosters client collaboration, enhancing scalability and adaptation in TransFed.
Our method incorporates a hyper network on the server, responsible for learning personalized projection matrices for
the focal modulation layers. This enables the generation
of client-specific keys, values, and queries.Furthermore, we
provide an analysis of adaptation bounds for TransFed using the learn-to-customize mechanism. Through intensive
experiments on datasets related to pneumonia classification, we demonstrate that TransFed, in combination with the
learn-to-tailor approach, achieves superior performance in
scenarios with non-IID data distributions, surpassing existing methods. Overall, TransFed paves the way for leveraging focal Modulation in federated learning, advancing the
capabilities of focal modulated transformer models in decentralized environments</p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <!-- Overview -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3"><b>Trans-Fed</b> Architecture</h2>
          <div class="content has-text-justified">
            <p>
              We introduce TransFed, an innovative federated learning framework built upon Focal modulation architecture. TransFed directly addresses the limitations of
FedAvg when applied to focal modulation in heterogeneous data scenarios. By facilitating the customization
of focal modulation for individual clients, TransFed significantly improves performance within the context
of tailored federated learning. Our proposal introduces a learn-to-tailor concept to enhance the utilization of client cooperation in the tailored layers. This mechanism aims to improve the
scalability and adaptation capabilities of TransFed.<br>
              <br>
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview -->
      <div class="columns is-centered">
        <img src="./static/images/arch.png" width="750" />
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Overview -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">TransFed Algorithm</h2>
            <div class="content has-text-justified">
              <p>

                <div class="columns is-centered">
                  <img src="./static/images/algorithm.png" width="400" />
                </div> 

                
              </p>
            </div>
          </div>
        </div class="columns is-centered">
        <!--/ Overview -->
      </div>
    </div>
  </section>

  <section class="hero is-dark">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Training Videos -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Quantitative Results</h2>
          </div>

        </div>

      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4"></h3>
            <div class="columns is-centered">
              <img src="./static/images/table.png" width="1000" />
            </div>
            <div class="content has-text-justified">
              <P>Table 2. demonstrates The TransFed method’s average test accuracy is computed alongside that of multiple transformer-based approaches, encompassing
different non-IID scenarios.
              </P>


            </div>




          </div>

        </div>
  </section>


  <section class="hero is-dark">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Generalization Videos -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"> Ablation Study </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">

            <div class="content has-text-justified">
              <p>

                <br>
              </p>
              <div class="columns is-centered">
                <img src="./static/images/Ablation.png" width="400" />
              </div>
              </div>
            </div>
          </div>
        </div>


      </div>
  </section>
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Overview -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"></h2>
            <div class="content has-text-justified">
              <p>
                
              </p>
            </div>
          </div>
        </div class="columns is-centered">
        <!--/ Overview -->
      </div>
    </div>
  </section>





  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>

              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered">

        </div>



      </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{ashraf2024transfed,
  title={TransFed: A way to epitomize Focal Modulation using Transformer-based Federated Learning},
  author={Ashraf, Tajamul and Bin Afzal Mir, Fuzayil and Gillani, Iqra Altaf},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={554--563},
  year={2024}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content">
          <p>
            Website template is inspired from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
