<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Multi-task learning with object point cloud representations enable strong generalization in dexterous manipulation.">
  <meta name="keywords" content="Dexterous Manipulation, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>D-MASTER: Mask Annealed Transformer for
    Unsupervised Domain Adaptation in Breast
    Cancer Detection from Mammograms</title>

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<style>
  .logo-container {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 10px;
  }
  .logo-container img {
      border: none;
      width: 160px; /* Set the width you want */
      height: auto; /* Maintain aspect ratio */
  }
  .content-between-logos {
      flex: 1;
      text-align: center;
  }
  .publication-details, .publication-links {
      margin: 10px 0;
  }
</style>

<body>
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><b>TransFed:</b>: A way to epitomize Focal Modulation using Transformer-based
Federated Learning</h1>
                    <h3 class="title is-4 conference-authors">
                        <div class="logo-container">
                            <a target="_blank" href="https://conferences.miccai.org/2024/en/">
                                <img src="static/images/miccai2023-logo.png" alt="MICCAI 2024">
                            </a>
                            <div class="content-between-logos">
                                <div class="publication-details">
                                    <div class="is-size-5 publication-authors">
                                        <span class="author-block">
                                            <a href="https://www.tajamulashraf.com/">Tajamul Ashraf</a><sup>1</sup>,</span>
                                        <span class="author-block">
                                            <a href="https://www.linkedin.com/in/fuzayil-mir/">Fuzayil Bin Afzal Mir</a><sup>2</sup>,
                                        </span>
                                            <a href="https://sites.google.com/nitsri.net/iqraaltaf">Iqra Altaf Gillani</a><sup>2</sup>,
                                        </span>
                                    </div>
                                    <div class="is-size-5 publication-authors">
                                        <span class="author-block"><sup>1</sup>Indian Institute of Technology Delhi,</span>
                                        <span class="author-block"><sup>1</sup>National Institute of Technology Delhi,</span>
                                    </div>
                                </div>
                                <div class="publication-links">
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2407.06585v1" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://github.com/Tajamul21/HF-Fed" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2407.06585v1" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>
                                    <span class="link-block">
                                        <a href="https://drive.google.com/drive/folders/1GT_1mkL2L_xcEA14375VSci2vQBWDh_h?usp=drive_link" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa fa-database"></i>
                                            </span>
                                            <span>Data</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                            <a target="_blank" href="https://deep-breath-miccai.github.io/#">
                                <img src="static/images/Deep-Breath-logo.png" alt="Deep Breath">
                            </a>
                        </div>
                    </h3>
                </div>
            </div>
        </div>
    </div>
</section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Federated learning has emerged as a promising
paradigm for collaborative machine learning, enabling
multiple clients to train a model while preserving data privacy jointly. Tailored federated learning takes this concept
further by accommodating client heterogeneity and facilitating the learning of personalized models. While the utilization of transformers within federated learning has attracted significant interest, there remains a need to investigate the effects of federated learning algorithms on the latest focal modulation-based transformers. In this paper, we
investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal
Modulation, particularly in scenarios with heterogeneous
data. To address this challenge, we propose TransFed, a
novel transformer-based federated learning framework that
not only aggregates model parameters but also learns tailored Focal Modulation for each client. Instead of employing a conventional customization mechanism that maintains client-specific focal modulation layers locally, we introduce a learn-to-tailor approach that fosters client collaboration, enhancing scalability and adaptation in TransFed.
Our method incorporates a hyper network on the server, responsible for learning personalized projection matrices for
the focal modulation layers. This enables the generation
of client-specific keys, values, and queries.Furthermore, we
provide an analysis of adaptation bounds for TransFed using the learn-to-customize mechanism. Through intensive
experiments on datasets related to pneumonia classification, we demonstrate that TransFed, in combination with the
learn-to-tailor approach, achieves superior performance in
scenarios with non-IID data distributions, surpassing existing methods. Overall, TransFed paves the way for leveraging focal Modulation in federated learning, advancing the
capabilities of focal modulated transformer models in decentralized environments</p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-widescreen">
      <!-- Overview -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3"><b>Trans-Fed</b> Architecture</h2>
          <div class="content has-text-justified">
            <p>
              We introduce TransFed, an innovative federated learning framework built upon Focal modulation architecture. TransFed directly addresses the limitations of
FedAvg when applied to focal modulation in heterogeneous data scenarios. By facilitating the customization
of focal modulation for individual clients, TransFed significantly improves performance within the context
of tailored federated learning. Our proposal introduces a learn-to-tailor concept to enhance the utilization of client cooperation in the tailored layers. This mechanism aims to improve the
scalability and adaptation capabilities of TransFed.<br>
              <br>
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview -->
      <div class="columns is-centered">
        <img src="./static/images/arch.png" width="750" />
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Overview -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Network of Networks (NoN)</h2>
            <div class="content has-text-justified">
              <p>
                In this paper, we
investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal
Modulation, particularly in scenarios with heterogeneous
data. To address this challenge, we propose TransFed, a
novel transformer-based federated learning framework that
not only aggregates model parameters but also learns tailored Focal Modulation for each client. 
              </p>
            </div>
          </div>
        </div class="columns is-centered">
        <!--/ Overview -->
      </div>
    </div>
  </section>

  <section class="hero is-dark">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Training Videos -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Quantitative Results</h2>
          </div>

        </div>

      </div>
    </div>
  </section>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4"></h3>
            <div class="columns is-centered">
              <img src="./static/images/table1.png" width="900" />
            </div>
            <div class="content has-text-justified">
              <P>Table 1 demonstrates that FL-based methods significantly improve
                performance with larger datasets, mitigating the non-iid problem. HF-Fed consistently delivers high
                performance across different dataset sizes, enhancing imaging quality effectively. All methods benefit
                from larger training samples, with
                HF-Fed remaining competitive. Our hypernetwork uses X-Ray geometry parameters to modulate feature maps,
                balancing stability and imaging performance,
                proving effective in achieving consistent and competitive result
              </P>


            </div>




          </div>

        </div>
  </section>


  <section class="hero is-dark">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Generalization Videos -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"> Ablation Study </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">

            <div class="content has-text-justified">
              <p>

                <br>
              </p>
              <div class="columns is-centered">
                <img src="./static/images/plot.png" width="900" />
              </div>
            </div>
          </div>
        </div>


      </div>
  </section>
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- Overview -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"></h2>
            <div class="content has-text-justified">
              <p>
                We conducted experiments on RSNA and Kermany datasets, varying the parameter alpha of the Beta distribution. Smaller alpha values indicate a higher level of heterogeneity. TransFed consistently outperformed benchmark
methods (FedAvg-T, FedBN [16], and pFedHN [9]) in handling label distribution heterogeneity. It demonstrated robustness even when other methods struggled to utilize client
heterogeneity effectively. We also explored the impact of
noise-induced feature imbalance on TransFed. By adding
Gaussian noise with increasing levels to each client, we
assessed TransFed’s performance. It consistently outperformed other methods in handling client-specific noise. The
number of focal-modulation blocks in TransFed was investigated, showing that increasing the number improved the
model’s ability to capture data heterogeneity and enhance
overall performance (Table 3). Consequently, we selected
eight as the default attention block number for TransFed in
subsequent experiments. We examined the impact of the
number of participating clients on model performance by
varying the sample rate. TransFed exhibited greater stability than FedAvg-T, as shown in Figure 3. This highlights
the robustness of TransFed in handling different client participation rates.
              </p>
            </div>
          </div>
        </div class="columns is-centered">
        <!--/ Overview -->
      </div>
    </div>
  </section>





  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>

              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered">

        </div>



      </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{ashraf2024transfed,
  title={TransFed: A way to epitomize Focal Modulation using Transformer-based Federated Learning},
  author={Ashraf, Tajamul and Bin Afzal Mir, Fuzayil and Gillani, Iqra Altaf},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={554--563},
  year={2024}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content">
          <p>
            Website template is inspired from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
